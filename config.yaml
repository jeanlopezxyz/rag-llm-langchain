# config.yaml - Configurado para modelo LLaMA local
llm_providers:
  - name: "Hugging Face"
    url: "http://localhost:52088/v1"
    enabled: true
    models:
      - name: "ibm-granite-granite-3.3-8b-instruct-GGUF"
        enabled: true
        weight: 1
        params:
          - name: "temperature"
            value: 0.7
          - name: "max_new_tokens"
            value: 1024
          - name: "top_p"
            value: 0.9

  - name: "OpenAI"
    url: "https://api.openai.com/v1"
    enabled: false
    models:
      - name: "gpt-3.5-turbo"
        enabled: false
        weight: 1
        params:
          - name: "temperature"
            value: 0.7
          - name: "max_new_tokens"
            value: 1024

default_provider: "Hugging Face"
default_model: "ibm-granite-granite-3.3-8b-instruct-GGUF"
type: "all"