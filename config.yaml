# config.yaml - Configurado para modelo LLaMA local
llm_providers:
  - name: "OpenShift AI (vLLM)"
    url: "http://host.containers.internal:57070/v1"
    enabled: true
    models:
      - name: "ibm-granite-granite-3.3-8b-instruct-GGUF"
        enabled: true
        weight: 1
        params:
          # --- MODIFICACIÓN: Se reemplaza 'repetition_penalty' por 'frequency_penalty' ---
          - name: "temperature"
            value: 0.01
          - name: "max_new_tokens"
            value: 512
          - name: "top_p"
            value: 0.95
          - name: "frequency_penalty"
            value: 0.5 # Un valor pequeño para penalizar repeticiones
  - name: "OpenAI"
    url: "https://api.openai.com/v1"
    enabled: false
    models:
      - name: "gpt-3.5-turbo"
        enabled: false
        weight: 1
        params:
          - name: "temperature"
            value: 0.7
          - name: "max_new_tokens"
            value: 1024

default_provider: "OpenShift AI (vLLM)"
default_model: "ibm-granite-granite-3.3-8b-instruct-GGUF"
type: "all"
